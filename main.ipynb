{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Crf8Z35IntSsI0IkbKzKPF7IxP0ffFv1",
      "authorship_tag": "ABX9TyNGqIdOzD0WII39em0dlftb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinath9121/SOLAR/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkMzDAaCGhAG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Installs the specific libraries needed for Physics-Informed Solar\"\"\"\n",
        "    packages = [\n",
        "        \"numpy\",\n",
        "        \"pandas\",\n",
        "        \"matplotlib\",\n",
        "        \"tensorflow\",   # The AI Brain\n",
        "        \"pvlib\",        # The Physics Engine (Crucial for Hyderabad calculations)\n",
        "        \"scikit-learn\",\n",
        "        \"opencv-python\", # For Image Processing\n",
        "        \"requests\"      # For downloading NASA data\n",
        "    ]\n",
        "    print(\"Installing libraries... (This may take a few minutes)\")\n",
        "    for package in packages:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "    print(\"✅ All libraries installed!\")\n",
        "\n",
        "def create_structure():\n",
        "    \"\"\"Creates the Two-Headed Project Structure\"\"\"\n",
        "\n",
        "    # Define the main project folders\n",
        "    folders = [\n",
        "        \"data/raw/stanford_skippd\",    # Head 2 Data (Power)\n",
        "        \"data/raw/singapore_swimcat\",  # Head 1 Data (Clouds)\n",
        "        \"data/processed\",              # Where we save 'normalized' data\n",
        "        \"data/bhavans_college\",        # <-- YOUR DEPLOYMENT SITE\n",
        "        \"models/checkpoints\",          # Save the best models here\n",
        "        \"src/physics\",                 # Where the Physics Loss code lives\n",
        "        \"src/layers\",                  # Where the Multi-Head code lives\n",
        "        \"notebooks\"                    # For your experiments\n",
        "    ]\n",
        "\n",
        "    base_dir = os.getcwd()\n",
        "\n",
        "    print(f\"\\nCreating Project Structure in: {base_dir}\")\n",
        "\n",
        "    for folder in folders:\n",
        "        path = os.path.join(base_dir, folder)\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        print(f\"Created: {folder}\")\n",
        "\n",
        "    # Create the Config File (Crucial for 'Generalization')\n",
        "    config_content = \"\"\"\n",
        "# PROJECT CONFIGURATION\n",
        "# ---------------------\n",
        "\n",
        "# 1. Location Settings (The Generalization Target)\n",
        "SITE_NAME = \"Bhavans Vivekananda College\"\n",
        "LATITUDE = 17.48  # Hyderabad\n",
        "LONGITUDE = 78.53\n",
        "ALTITUDE = 540    # Meters\n",
        "\n",
        "# 2. Physics Constraints\n",
        "MAX_THEORETICAL_POWER = 30.0 # kW (Adjust this to your college's actual capacity)\n",
        "PANEL_TILT = 15              # Degrees (Standard for Hyderabad)\n",
        "\n",
        "# 3. Model Settings\n",
        "IMAGE_SIZE = (64, 64)        # Fish-eye image resize\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "# 4. Cloud Classes (from SWIMCAT)\n",
        "CLASSES = ['Clear', 'Patterned', 'Thick Dark', 'Thick White', 'Veil']\n",
        "\"\"\"\n",
        "\n",
        "    with open(\"config.py\", \"w\") as f:\n",
        "        f.write(config_content)\n",
        "    print(\"✅ Created config.py with Hyderabad coordinates\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- PINN SOLAR PROJECT SETUP ---\")\n",
        "    create_structure()\n",
        "    user_input = input(\"\\nDo you want to install the libraries now? (y/n): \")\n",
        "    if user_input.lower() == 'y':\n",
        "        install_requirements()\n",
        "\n",
        "    print(\"\\n--- SETUP COMPLETE ---\")\n",
        "    print(\"Next Step: Download the datasets into 'data/raw/'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets h5py\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "# Load the benchmark dataset directly from the repo\n",
        "# This avoids the 404 error because it uses the Hugging Face API\n",
        "print(\"Connecting to SolarBench...\")\n",
        "dataset = load_dataset(\"solarbench/SKIPPD\")\n",
        "\n",
        "# The data is already split into 'train' and 'test' for you\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']\n",
        "\n",
        "print(f\"✅ Success! Loaded {len(train_data)} training samples.\")"
      ],
      "metadata": {
        "id": "4iQa0if5Hejc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pvlib\n",
        "from pvlib.location import Location\n",
        "\n",
        "# 1. Load a sample of your SKIPP'D data\n",
        "# (Assuming you have downloaded the CSV)\n",
        "# df = pd.read_csv('path/to/skippd_2017.csv')\n",
        "\n",
        "# MOCK DATA for example (Replace this with your real data)\n",
        "data = {\n",
        "    'datetime': ['2017-06-01 12:00:00', '2017-12-01 12:00:00'], # Summer vs Winter\n",
        "    'actual_power_kw': [28.5, 15.2]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Convert to DateTime Object\n",
        "df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "# 3. CRITICAL: Set Timezone to UTC (SKIPP'D is usually UTC)\n",
        "df = df.set_index('datetime').tz_localize('UTC')\n",
        "\n",
        "# 4. Convert to \"Solar Position\" (The Physics Input)\n",
        "# Stanford Coordinates (approx)\n",
        "lat, lon = 37.42, -122.17\n",
        "site = Location(lat, lon, tz='UTC')\n",
        "\n",
        "# Calculate where the sun was for these specific timestamps\n",
        "solpos = site.get_solarposition(df.index)\n",
        "\n",
        "print(\"--- PHYSICS CHECK ---\")\n",
        "print(solpos[['apparent_zenith', 'elevation']])\n",
        "\n",
        "# Zenith > 80 means \"Night/Twilight\" -> FILTER THESE OUT"
      ],
      "metadata": {
        "id": "JpZN9tGaL6uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GhOnSKhsM_4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38a07d71"
      },
      "source": [
        "# Task\n",
        "Inspect the structure of the `train_data` dataset, identify the time-related column, extract the minimum and maximum datetime values from this column to determine the dataset's timeline, and present the timeline to the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3363130"
      },
      "source": [
        "## Inspect Dataset Structure\n",
        "\n",
        "### Subtask:\n",
        "Examine the structure of the `train_data` dataset to identify columns containing time-related information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f443959"
      },
      "source": [
        "**Reasoning**:\n",
        "To understand the structure of the `train_data` dataset, I will first print its features (column names) and then display the first element to examine the data types and content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10362afe"
      },
      "source": [
        "print(\"Keys in train_data:\", train_data.features.keys())\n",
        "print(\"\\nFirst element of train_data:\")\n",
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5757a617"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the inspection, the 'time' column is identified as the time-related information. To determine the dataset's timeline, I will extract the minimum and maximum datetime values from this column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6d8e4d9"
      },
      "source": [
        "min_datetime = min(item['time'] for item in train_data)\n",
        "max_datetime = max(item['time'] for item in train_data)\n",
        "\n",
        "print(f\"\\nDataset Timeline:\\nMinimum datetime: {min_datetime}\\nMaximum datetime: {max_datetime}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_years(example):\n",
        "    year = example['time'].year\n",
        "    return year not in [2018, 2019]\n",
        "\n",
        "original_num_samples = len(train_data)\n",
        "\n",
        "train_data = train_data.filter(filter_years)\n",
        "\n",
        "filtered_num_samples = len(train_data)\n",
        "print(f\"Original number of samples: {original_num_samples}\")\n",
        "print(f\"Number of samples after filtering 2018 and 2019 data: {filtered_num_samples}\")\n",
        "\n",
        "# Verify the new timeline\n",
        "min_datetime_filtered = min(item['time'] for item in train_data)\n",
        "max_datetime_filtered = max(item['time'] for item in train_data)\n",
        "print(f\"\\nNew Dataset Timeline:\\nMinimum datetime: {min_datetime_filtered}\\nMaximum datetime: {max_datetime_filtered}\")"
      ],
      "metadata": {
        "id": "O5PtQnOTfkL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pvlib\n",
        "from pvlib.location import Location\n",
        "\n",
        "# 1. Load a sample of your SKIPP'D data\n",
        "# (Assuming you have downloaded the CSV)\n",
        "# df = pd.read_csv('path/to/skippd_2017.csv')\n",
        "\n",
        "# MOCK DATA for example (Replace this with your real data)\n",
        "data = {\n",
        "    'datetime': ['2017-06-01 12:00:00', '2017-12-01 12:00:00'], # Summer vs Winter\n",
        "    'actual_power_kw': [28.5, 15.2]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2. Convert to DateTime Object\n",
        "df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "\n",
        "# 3. CRITICAL: Set Timezone to UTC (SKIPP'D is usually UTC)\n",
        "df = df.set_index('datetime').tz_localize('UTC')\n",
        "\n",
        "# 4. Convert to \"Solar Position\" (The Physics Input)\n",
        "# Stanford Coordinates (approx)\n",
        "lat, lon = 37.42, -122.17\n",
        "site = Location(lat, lon, tz='UTC')\n",
        "\n",
        "# Calculate where the sun was for these specific timestamps\n",
        "solpos = site.get_solarposition(df.index)\n",
        "\n",
        "print(\"--- PHYSICS CHECK ---\")\n",
        "print(solpos[['apparent_zenith', 'elevation']])\n",
        "\n",
        "# Zenith > 80 means \"Night/Twilight\" -> FILTER THESE OUT"
      ],
      "metadata": {
        "id": "YA6pVU1rQfHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pvlib.location import Location\n",
        "\n",
        "# 1. Stanford Location\n",
        "lat, lon = 37.42, -122.17\n",
        "site = Location(lat, lon, tz='UTC')\n",
        "\n",
        "# 2. Check \"California Noon\" (which is 20:00 UTC)\n",
        "times = pd.to_datetime(['2017-06-01 20:00:00', '2017-12-01 20:00:00']).tz_localize('UTC')\n",
        "\n",
        "# 3. Calculate Physics\n",
        "solpos = site.get_solarposition(times)\n",
        "\n",
        "print(\"--- CORRECTED PHYSICS CHECK (California Noon) ---\")\n",
        "print(solpos[['apparent_zenith', 'elevation']])"
      ],
      "metadata": {
        "id": "g9ZtRZayQfvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Check if the file exists\n",
        "file_path = \"train_master_physics.csv\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    print(\"✅ GREAT NEWS: The file was created successfully!\")\n",
        "\n",
        "    # 2. Load the first 5 rows\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # 3. SHOW ME THE NUMBERS (Crucial Step)\n",
        "    print(\"\\n--- UNIT CHECK ---\")\n",
        "    print(df[['datetime', 'power', 'GHI_limit', 'target_k']].head())\n",
        "\n",
        "    # 4. Automatic Advice\n",
        "    power_val = df['power'].iloc[0]\n",
        "    ghi_val = df['GHI_limit'].iloc[0]\n",
        "\n",
        "    if power_val > 1000 and ghi_val < 2:\n",
        "        print(\"\\n⚠️ ALERT: Unit Mismatch! Power is in Watts (e.g., 24000) but Physics is in kW (e.g., 0.9).\")\n",
        "    elif power_val < 100 and ghi_val > 100:\n",
        "        print(\"\\n⚠️ ALERT: Unit Mismatch! Power is in kW (e.g., 24) but Physics is in Watts (e.g., 900).\")\n",
        "    else:\n",
        "        print(\"\\n✅ UNITS LOOK GOOD! We are ready for images.\")\n",
        "else:\n",
        "    print(\"⏳ File not found yet. The previous cell is still downloading/processing. Please wait 2 more minutes.\")"
      ],
      "metadata": {
        "id": "6WV7o9RxrETv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pvlib\n",
        "from pvlib.location import Location\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "# 1. SETUP: Stanford Coordinates\n",
        "lat, lon = 37.42, -122.17\n",
        "site = Location(lat, lon, tz='Etc/GMT+8')\n",
        "\n",
        "print(\"1. Loading SKIPP'D Dataset (Lazy Mode)...\")\n",
        "# We load the dataset pointer, but NOT the data itself yet\n",
        "dataset = load_dataset(\"solarbench/skippd\", split=\"train\")\n",
        "\n",
        "# 2. THE RAM FIX: Extract ONLY Time and Power\n",
        "# We explicitly skip the 'image' column here.\n",
        "print(\"2. Extracting Metadata (Dropping Images to save RAM)...\")\n",
        "df_phys = pd.DataFrame({\n",
        "    'datetime': dataset['time'], # Extract list of times\n",
        "    'power': dataset['pv']       # Extract list of power values\n",
        "})\n",
        "\n",
        "# 3. CLEANING & FORMATTING\n",
        "print(f\"   Raw Rows: {len(df_phys)}\")\n",
        "# Convert to datetime objects\n",
        "df_phys['datetime'] = pd.to_datetime(df_phys['datetime'])\n",
        "\n",
        "# Timezone Fix (UTC -> California)\n",
        "if df_phys['datetime'].dt.tz is None:\n",
        "    df_phys['datetime'] = df_phys['datetime'].dt.tz_localize('UTC').dt.tz_convert('Etc/GMT+8')\n",
        "else:\n",
        "    df_phys['datetime'] = df_phys['datetime'].dt.tz_convert('Etc/GMT+8')\n",
        "\n",
        "# Set Index for Physics Engine\n",
        "df_phys = df_phys.set_index('datetime')\n",
        "\n",
        "# 4. FILTER: 2017 ONLY + DAYLIGHT ONLY\n",
        "print(\"3. Applying Filters (2017 + Daylight)...\")\n",
        "# Filter Year 2017\n",
        "df_phys = df_phys[df_phys.index.year == 2017]\n",
        "# Filter Hours (9 AM - 4 PM)\n",
        "df_phys = df_phys[(df_phys.index.hour >= 9) & (df_phys.index.hour < 16)]\n",
        "\n",
        "print(f\"   Rows after filtering: {len(df_phys)}\")\n",
        "\n",
        "# 5. PHYSICS ENGINE (Calculate Limits)\n",
        "print(\"4. Calculating Physics Limits...\")\n",
        "solpos = site.get_solarposition(df_phys.index)\n",
        "clearsky = site.get_clearsky(df_phys.index)\n",
        "\n",
        "df_phys['GHI_limit'] = clearsky['ghi'].values\n",
        "df_phys['Zenith'] = solpos['apparent_zenith'].values\n",
        "\n",
        "# 6. CREATE TARGET (The Ratio)\n",
        "# Remove night/low light\n",
        "df_phys = df_phys[df_phys['GHI_limit'] > 10]\n",
        "# Calculate Ratio\n",
        "df_phys['target_k'] = df_phys['power'] / df_phys['GHI_limit']\n",
        "df_phys['target_k'] = df_phys['target_k'].clip(0, 1.2)\n",
        "\n",
        "# 7. SAVE\n",
        "output_filename = \"train_master_physics.csv\"\n",
        "# Reset index to save datetime as a column\n",
        "final_df = df_phys.reset_index()[['datetime', 'power', 'GHI_limit', 'target_k']]\n",
        "final_df.to_csv(output_filename, index=False)\n",
        "\n",
        "print(f\"\\n✅ SUCCESS! Saved '{output_filename}'.\")\n",
        "print(\"\\n--- UNIT CHECK (Please paste these numbers) ---\")\n",
        "print(final_df.head())"
      ],
      "metadata": {
        "id": "VZe2JEelr4OH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. SETUP\n",
        "CSV_PATH = \"train_master_physics.csv\" # The file you just created\n",
        "OUTPUT_PATH = \"data/processed/X_images_64.npy\"\n",
        "IMG_SIZE = 64\n",
        "\n",
        "# 2. LOAD RESOURCES\n",
        "print(\"1. Loading Master CSV...\")\n",
        "try:\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "    # Ensure datetime is parsed correctly to match dataset\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "    # We need a quick lookup for valid timestamps\n",
        "    valid_times = set(df['datetime'].astype(str).values)\n",
        "    print(f\"   Target Images to Process: {len(df)}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"❌ ERROR: Could not find train_master_physics.csv. Did the previous step finish?\")\n",
        "    exit()\n",
        "\n",
        "print(\"2. Loading SKIPP'D Dataset (Images)...\")\n",
        "# We load the same dataset again to grab the images\n",
        "dataset = load_dataset(\"solarbench/skippd\", split=\"train\")\n",
        "\n",
        "# 3. PROCESSING LOOP\n",
        "print(f\"3. Resizing images to {IMG_SIZE}x{IMG_SIZE}...\")\n",
        "\n",
        "# We will store images in a lightweight list first\n",
        "processed_images = []\n",
        "keep_indices = []\n",
        "\n",
        "# Iterate through the dataset and pick ONLY the rows that matched our Physics Filter\n",
        "for i, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
        "\n",
        "    # Check if this image's time is in our \"Valid\" list\n",
        "    # Note: We need to match the string format exactly.\n",
        "    # SKIPP'D 'time' is usually YYYY-MM-DD HH:MM:SS\n",
        "\n",
        "    # Convert HF dataset time to standard string\n",
        "    sample_time = pd.to_datetime(sample['time']).tz_localize('UTC').tz_convert('Etc/GMT+8')\n",
        "    time_str = str(sample_time)\n",
        "\n",
        "    if time_str in valid_times:\n",
        "        # It's a valid daylight image! Process it.\n",
        "\n",
        "        # Get Image (PIL Format)\n",
        "        img = sample['image']\n",
        "\n",
        "        # Resize (High quality downsampling)\n",
        "        img_small = img.resize((IMG_SIZE, IMG_SIZE), Image.Resampling.LANCZOS)\n",
        "\n",
        "        # Convert to Array (Keep as uint8 0-255 to save RAM)\n",
        "        img_array = np.array(img_small, dtype=np.uint8)\n",
        "\n",
        "        processed_images.append(img_array)\n",
        "        keep_indices.append(i)\n",
        "\n",
        "# 4. CONVERT & SAVE\n",
        "print(\"4. Saving to efficient .npy file...\")\n",
        "X_images = np.array(processed_images)\n",
        "\n",
        "# Final Shape Check\n",
        "print(f\"   Final Image Shape: {X_images.shape}\")\n",
        "# Should be (59107, 64, 64, 3)\n",
        "\n",
        "# Save\n",
        "os.makedirs(\"data/processed\", exist_ok=True)\n",
        "np.save(OUTPUT_PATH, X_images)\n",
        "\n",
        "print(f\"✅ SUCCESS! Images saved to {OUTPUT_PATH}\")\n",
        "print(f\"   Size on disk: {os.path.getsize(OUTPUT_PATH) / (1024*1024):.2f} MB\")"
      ],
      "metadata": {
        "id": "4qFr1R-Ukp_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load the file you just made\n",
        "df = pd.read_csv(\"train_master_physics.csv\")\n",
        "\n",
        "# 2. Check the Max Value before fixing\n",
        "max_val_before = df['target_k'].max()\n",
        "print(f\"Max Ratio before fix: {max_val_before:.4f} (Too small!)\")\n",
        "\n",
        "# 3. THE FIX: Normalize by the 99th Percentile\n",
        "# (We use 99th % instead of Max to ignore sensor glitches/outliers)\n",
        "scaling_factor = np.percentile(df['target_k'], 99)\n",
        "print(f\"Scaling Factor (System Efficiency Proxy): {scaling_factor:.4f}\")\n",
        "\n",
        "df['target_k'] = df['target_k'] / scaling_factor\n",
        "\n",
        "# Clip to 0-1 range (Physics enforcement)\n",
        "df['target_k'] = df['target_k'].clip(0, 1.0)\n",
        "\n",
        "# 4. Save the \"Fixed\" version\n",
        "df.to_csv(\"train_master_physics.csv\", index=False)\n",
        "\n",
        "print(\"\\n✅ UNITS FIXED! The AI will now learn correctly.\")\n",
        "print(df[['datetime', 'power', 'target_k']].head())"
      ],
      "metadata": {
        "id": "EPVyW0ngtuxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. SETUP\n",
        "CSV_PATH = \"train_master_physics.csv\"\n",
        "OUTPUT_PATH = \"data/processed/X_images_64.npy\"\n",
        "IMG_SIZE = 64\n",
        "\n",
        "# 2. LOAD RESOURCES\n",
        "print(\"1. Reading your Fixed CSV...\")\n",
        "# Ensure the CSV exists\n",
        "if not os.path.exists(CSV_PATH):\n",
        "    print(f\"❌ ERROR: {CSV_PATH} not found. Did Step 1 finish successfully?\")\n",
        "else:\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "    # Create a \"Set\" of valid times for instant lookup (Fast!)\n",
        "    # We ensure uniformity by converting everything to string format\n",
        "    print(\"   Building valid timestamp index...\")\n",
        "    valid_times = set(pd.to_datetime(df['datetime']).astype(str).values)\n",
        "    print(f\"   We need to find {len(valid_times)} matching images.\")\n",
        "\n",
        "    print(\"2. Opening Image Stream (Lazy Load)...\")\n",
        "    dataset = load_dataset(\"solarbench/SKIPPD\", split=\"train\")\n",
        "\n",
        "    # 3. PROCESSING LOOP\n",
        "    print(f\"3. Processing images to {IMG_SIZE}x{IMG_SIZE}...\")\n",
        "    processed_images = []\n",
        "    matched_count = 0\n",
        "\n",
        "    for sample in tqdm(dataset):\n",
        "        try:\n",
        "            # --- THE FIX IS HERE ---\n",
        "            # 1. Convert to pandas Timestamp\n",
        "            ts = pd.to_datetime(sample['time'])\n",
        "\n",
        "            # 2. Smart Timezone Handling\n",
        "            if ts.tz is None:\n",
        "                # If it has NO timezone, assume UTC then convert\n",
        "                ts = ts.tz_localize('UTC').tz_convert('Etc/GMT+8')\n",
        "            else:\n",
        "                # If it ALREADY has a timezone, just convert\n",
        "                ts = ts.tz_convert('Etc/GMT+8')\n",
        "\n",
        "            # 3. Match format\n",
        "            ts_str = str(ts)\n",
        "\n",
        "            if ts_str in valid_times:\n",
        "                # FOUND A MATCH!\n",
        "\n",
        "                # Resize Image\n",
        "                img = sample['image'].resize((IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "                # Convert to simple numbers (0-255) to save space\n",
        "                img_arr = np.array(img, dtype=np.uint8)\n",
        "\n",
        "                processed_images.append(img_arr)\n",
        "                matched_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            # We print the error but don't stop the loop\n",
        "            # This handles occasional bad data points\n",
        "            continue\n",
        "\n",
        "    # 4. SAVE\n",
        "    print(f\"\\n4. Saving {matched_count} images to disk...\")\n",
        "    os.makedirs(\"data/processed\", exist_ok=True)\n",
        "    X_images = np.array(processed_images)\n",
        "\n",
        "    np.save(OUTPUT_PATH, X_images)\n",
        "\n",
        "    print(f\"✅ DONE! Saved images to {OUTPUT_PATH}\")\n",
        "    print(f\"   Final Shape: {X_images.shape}\")"
      ],
      "metadata": {
        "id": "Loog7PuZt8t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6H3f5_9uiuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PHASE 2"
      ],
      "metadata": {
        "id": "TGviRaQnwob4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. SETUP\n",
        "# SWIMCAT_URL = \"http://vintage.winklerbros.net/Publications/swimcat/SWIMCAT.zip\" # No longer needed\n",
        "SAVE_DIR = \"/content/drive/MyDrive/COLAB DATASET/swimcat\" # Updated to user's local path\n",
        "PROCESSED_PATH_X = \"data/processed/X_swimcat.npy\"\n",
        "PROCESSED_PATH_Y = \"data/processed/y_swimcat.npy\"\n",
        "IMG_SIZE = 64\n",
        "\n",
        "# Cloud Categories (The 5 Classes from your Presentation)\n",
        "CLASSES = ['Clear sky', 'Patterned clouds', 'Thick dark clouds', 'Thick white clouds', 'Veil clouds']\n",
        "# We map them to numbers: 0, 1, 2, 3, 4\n",
        "class_map = {name: i for i, name in enumerate(CLASSES)}\n",
        "\n",
        "def download_and_extract():\n",
        "    print(\"1. Downloading SWIMCAT Dataset (Compact)...\")\n",
        "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "    # Download\n",
        "    response = requests.get(SWIMCAT_URL)\n",
        "    if response.status_code == 200:\n",
        "        with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
        "            zip_ref.extractall(SAVE_DIR)\n",
        "        print(\"✅ Downloaded and Unzipped!\")\n",
        "    else:\n",
        "        print(f\"❌ Failed to download. Status: {response.status_code}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def process_swimcat():\n",
        "    print(\"2. Processing SWIMCAT Images...\")\n",
        "    X_data = []\n",
        "    y_data = []\n",
        "\n",
        "    # Walk through the folders\n",
        "    # The zip usually extracts to a folder structure\n",
        "    base_path = SAVE_DIR # Now directly use SAVE_DIR as base_path\n",
        "\n",
        "    if not os.path.exists(base_path):\n",
        "        print(f\"❌ Error: The specified path '{base_path}' does not exist. Please ensure the dataset is there.\")\n",
        "        return\n",
        "\n",
        "    total_images = 0\n",
        "\n",
        "    for class_name in CLASSES:\n",
        "        class_path = os.path.join(base_path, class_name)\n",
        "        if not os.path.exists(class_path):\n",
        "            print(f\"⚠️ Warning: Could not find folder for {class_name} at {class_path}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"   Processing Class: {class_name}...\")\n",
        "\n",
        "        files = os.listdir(class_path)\n",
        "        for fname in files:\n",
        "            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(class_path, fname)\n",
        "                try:\n",
        "                    # Load and Resize\n",
        "                    img = Image.open(img_path).convert('RGB')\n",
        "                    img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "                    img_arr = np.array(img, dtype=np.uint8)\n",
        "\n",
        "                    X_data.append(img_arr)\n",
        "                    y_data.append(class_map[class_name])\n",
        "                    total_images += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "    # Convert to Arrays\n",
        "    X_final = np.array(X_data)\n",
        "    y_final = np.array(y_data)\n",
        "\n",
        "    print(f\"\\n3. Saving {total_images} labelled images...\")\n",
        "    os.makedirs(os.path.dirname(PROCESSED_PATH_X), exist_ok=True)\n",
        "    np.save(PROCESSED_PATH_X, X_final)\n",
        "    np.save(PROCESSED_PATH_Y, y_final)\n",
        "\n",
        "    print(f\"✅ SUCCESS! Saved SWIMCAT data.\")\n",
        "    print(f\"   X Shape: {X_final.shape}\")\n",
        "    print(f\"   y Shape: {y_final.shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Removed the call to download_and_extract()\n",
        "    process_swimcat()\n"
      ],
      "metadata": {
        "id": "ovX8rJRewtjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. MOUNT GOOGLE DRIVE\n",
        "print(\"1. Connecting to Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BASE_DIR = \"/content/drive/MyDrive/COLAB DATASET/swimcat\"\n",
        "PROCESSED_PATH_X = \"data/processed/X_swimcat.npy\"\n",
        "PROCESSED_PATH_Y = \"data/processed/y_swimcat.npy\"\n",
        "IMG_SIZE = 64\n",
        "\n",
        "# Exact Folder Names\n",
        "FOLDER_NAMES = ['A-sky', 'B-pattern', 'C-thick-dark', 'D-thick-white', 'E-veil']\n",
        "\n",
        "def process_swimcat_deep():\n",
        "    print(f\"2. Scanning folder: {BASE_DIR}\")\n",
        "\n",
        "    if not os.path.exists(BASE_DIR):\n",
        "        print(f\"❌ ERROR: Could not find folder at {BASE_DIR}\")\n",
        "        return\n",
        "\n",
        "    X_data = []\n",
        "    y_data = []\n",
        "    total_images = 0\n",
        "\n",
        "    for class_id, folder_name in enumerate(FOLDER_NAMES):\n",
        "        # 1. Enter the Category Folder (e.g., \"A-sky\")\n",
        "        category_path = os.path.join(BASE_DIR, folder_name)\n",
        "\n",
        "        # 2. Enter the \"images\" Subfolder (e.g., \"A-sky/images\")\n",
        "        # We check if \"images\" exists; if not, we try the category folder itself\n",
        "        image_subfolder = os.path.join(category_path, \"images\")\n",
        "\n",
        "        if os.path.exists(image_subfolder):\n",
        "            target_path = image_subfolder\n",
        "        else:\n",
        "            target_path = category_path # Fallback if photos are not in subfolder\n",
        "\n",
        "        print(f\"   Processing Class {class_id} ({folder_name}) in: {target_path}\")\n",
        "\n",
        "        if not os.path.exists(target_path):\n",
        "            print(f\"   ⚠️ WARNING: Path not found: {target_path}\")\n",
        "            continue\n",
        "\n",
        "        files = os.listdir(target_path)\n",
        "\n",
        "        for fname in files:\n",
        "            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(target_path, fname)\n",
        "                try:\n",
        "                    # Load & Resize\n",
        "                    img = Image.open(img_path).convert('RGB')\n",
        "                    img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "                    img_arr = np.array(img, dtype=np.uint8)\n",
        "\n",
        "                    X_data.append(img_arr)\n",
        "                    y_data.append(class_id)\n",
        "                    total_images += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"   Error reading {fname}: {e}\")\n",
        "\n",
        "    if total_images == 0:\n",
        "        print(\"❌ ERROR: Still found 0 images. Please check the paths.\")\n",
        "        return\n",
        "\n",
        "    # Save\n",
        "    print(f\"\\n3. Saving {total_images} labelled images...\")\n",
        "    os.makedirs(\"data/processed\", exist_ok=True)\n",
        "    np.save(PROCESSED_PATH_X, np.array(X_data))\n",
        "    np.save(PROCESSED_PATH_Y, np.array(y_data))\n",
        "\n",
        "    print(f\"✅ SUCCESS! Saved SWIMCAT data.\")\n",
        "    print(f\"   Final Shape: {np.array(X_data).shape}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_swimcat_deep()"
      ],
      "metadata": {
        "id": "kkfIn14E3xvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# 1. LOAD DATA (The SWIMCAT data you just saved)\n",
        "print(\"1. Loading SWIMCAT Data...\")\n",
        "X = np.load(\"data/processed/X_swimcat.npy\")\n",
        "y = np.load(\"data/processed/y_swimcat.npy\")\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "X = X.astype('float32') / 255.0\n",
        "\n",
        "# Split into Train (80%) and Test (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"   Training on {len(X_train)} images, Testing on {len(X_test)} images.\")\n",
        "\n",
        "# 2. BUILD THE CLOUD CLASSIFIER (Head 1)\n",
        "# This is a standard CNN architecture\n",
        "model = models.Sequential([\n",
        "    # Layer 1: The \"Eye\" (Conv2D)\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Layer 2: Deeper Features\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Layer 3: Complex Patterns\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "\n",
        "    # Flatten: Turn 2D images into 1D numbers\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Dense Layers: The \"Brain\"\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(5, activation='softmax') # 5 Output Classes (Clear, Pattern, Dark, etc.)\n",
        "])\n",
        "\n",
        "# 3. COMPILE\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. TRAIN (It's small, so 15 epochs is plenty)\n",
        "print(\"\\n2. Training Cloud Classifier...\")\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=15,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    verbose=1)\n",
        "\n",
        "# 5. SAVE THE BRAIN\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "model.save(\"models/cloud_classifier.h5\")\n",
        "\n",
        "print(\"\\n✅ SUCCESS! 'Cloud Brain' trained and saved.\")\n",
        "print(f\"   Final Accuracy: {history.history['val_accuracy'][-1]*100:.2f}%\")"
      ],
      "metadata": {
        "id": "B99c7uUJ4B8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Model, Input\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. LOAD THE DATA INGREDIENTS\n",
        "print(\"1. Loading Processed Data...\")\n",
        "# Head 1 Data (Images)\n",
        "X_img = np.load(\"data/processed/X_images_64.npy\").astype('float32') / 255.0\n",
        "# Head 2 Data (Physics)\n",
        "df = pd.read_csv(\"train_master_physics.csv\")\n",
        "\n",
        "# We need to align them perfectly\n",
        "# (The image script saved all matching images, so indices should match)\n",
        "# Let's be safe and take the first N matching rows\n",
        "min_len = min(len(X_img), len(df))\n",
        "X_img = X_img[:min_len]\n",
        "df = df.iloc[:min_len]\n",
        "\n",
        "print(f\"   Aligned Samples: {min_len}\")\n",
        "\n",
        "# 2. PREPARE INPUTS & TARGETS\n",
        "# Input 2: Physics Limits (The Guardrail)\n",
        "# We normalize the Limit so the Neural Net understands it easily\n",
        "scaler = StandardScaler()\n",
        "X_phys = scaler.fit_transform(df[['GHI_limit']].values)\n",
        "\n",
        "# Target: The Ratio (target_k)\n",
        "y = df['target_k'].values\n",
        "\n",
        "# Split (Train/Test)\n",
        "# We must split ALL inputs together to keep them aligned\n",
        "X_img_train, X_img_test, X_phys_train, X_phys_test, y_train, y_test = train_test_split(\n",
        "    X_img, X_phys, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# [cite_start]3. DEFINE THE \"PHYSICS LOSS\" (Your Secret Sauce) [cite: 66-67]\n",
        "# This function forces the model to respect the \"GHI Limit\"\n",
        "def physics_guided_loss(y_true, y_pred):\n",
        "    # Standard Error (MSE) - \"Get the number right\"\n",
        "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "    # Physics Penalty (ReLU) - \"Don't predict negative power\"\n",
        "    # (y_pred should be > 0)\n",
        "    neg_penalty = tf.reduce_mean(tf.nn.relu(-y_pred))\n",
        "\n",
        "    # Upper Limit Penalty - \"Don't predict more than 100% of sun\"\n",
        "    # (y_pred should be < 1.0 roughly, since we normalized)\n",
        "    upper_penalty = tf.reduce_mean(tf.nn.relu(y_pred - 1.2))\n",
        "\n",
        "    # Total Loss = Error + Penalties\n",
        "    return mse + (0.5 * neg_penalty) + (0.5 * upper_penalty)\n",
        "\n",
        "# 4. BUILD THE TWO-HEADED MODEL\n",
        "print(\"2. Constructing the Architecture...\")\n",
        "\n",
        "# --- HEAD 1: THE CLOUD EYE (Pre-trained) ---\n",
        "# We load the brain you just trained\n",
        "base_cloud_model = tf.keras.models.load_model(\"models/cloud_classifier.h5\")\n",
        "\n",
        "# We remove the top layer (the classifier part)\n",
        "# We only want the \"Features\" (Dark vs Light patterns)\n",
        "cloud_features = Model(inputs=base_cloud_model.input,\n",
        "                       outputs=base_cloud_model.layers[-2].output)\n",
        "\n",
        "# FREEZE IT (Transfer Learning)\n",
        "# We don't want to break the cloud brain while training the power brain\n",
        "cloud_features.trainable = False\n",
        "\n",
        "# Input A: The Image\n",
        "input_img = Input(shape=(64, 64, 3), name=\"Image_Input\")\n",
        "visual_embedding = cloud_features(input_img)\n",
        "\n",
        "# --- HEAD 2: THE PHYSICS BRAIN ---\n",
        "# Input B: The Clear Sky Limit (Input 3)\n",
        "input_phys = Input(shape=(1,), name=\"Physics_Input\")\n",
        "phys_embedding = layers.Dense(16, activation='relu')(input_phys)\n",
        "\n",
        "# --- FUSION (Concatenate) ---\n",
        "# Combine \"What I see\" (Clouds) with \"What Physics says\" (Limit)\n",
        "combined = layers.Concatenate()([visual_embedding, phys_embedding])\n",
        "\n",
        "# Interpretation Layers\n",
        "z = layers.Dense(64, activation='relu')(combined)\n",
        "z = layers.Dropout(0.2)(z) # Prevent memorization\n",
        "z = layers.Dense(32, activation='relu')(z)\n",
        "\n",
        "# OUTPUT: The Power Ratio (0.0 to 1.0)\n",
        "output_k = layers.Dense(1, activation='linear', name=\"Power_Output\")(z)\n",
        "\n",
        "# Create the Full Model\n",
        "final_model = Model(inputs=[input_img, input_phys], outputs=output_k)\n",
        "\n",
        "# 5. COMPILE\n",
        "final_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                    loss=physics_guided_loss,\n",
        "                    metrics=['mae'])\n",
        "\n",
        "print(\"✅ SUCCESS! The Two-Headed Monster is built.\")\n",
        "final_model.summary()"
      ],
      "metadata": {
        "id": "lDBWbxjx6oi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Model, Input\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. LOAD THE DATA INGREDIENTS\n",
        "print(\"1. Loading Processed Data...\")\n",
        "X_img = np.load(\"data/processed/X_images_64.npy\").astype('float32') / 255.0\n",
        "df = pd.read_csv(\"train_master_physics.csv\")\n",
        "\n",
        "# ALIGNMENT CHECK\n",
        "min_len = min(len(X_img), len(df))\n",
        "X_img = X_img[:min_len]\n",
        "df = df.iloc[:min_len]\n",
        "print(f\"   Aligned Samples: {min_len}\")\n",
        "\n",
        "# 2. PREPARE INPUTS & TARGETS\n",
        "scaler = StandardScaler()\n",
        "X_phys = scaler.fit_transform(df[['GHI_limit']].values)\n",
        "y = df['target_k'].values\n",
        "\n",
        "# SPLIT\n",
        "X_img_train, X_img_test, X_phys_train, X_phys_test, y_train, y_test = train_test_split(\n",
        "    X_img, X_phys, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. DEFINE THE PHYSICS LOSS\n",
        "def physics_guided_loss(y_true, y_pred):\n",
        "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "    neg_penalty = tf.reduce_mean(tf.nn.relu(-y_pred))\n",
        "    upper_penalty = tf.reduce_mean(tf.nn.relu(y_pred - 1.2))\n",
        "    return mse + (0.5 * neg_penalty) + (0.5 * upper_penalty)\n",
        "\n",
        "# 4. BUILD THE MODEL (The Fix is Here)\n",
        "print(\"2. Constructing the Architecture...\")\n",
        "\n",
        "# Load the saved brain\n",
        "base_cloud_model = tf.keras.models.load_model(\"models/cloud_classifier.h5\")\n",
        "\n",
        "# --- THE FIX: MANUAL RECONSTRUCTION ---\n",
        "# Instead of slicing the model object, we create a new input\n",
        "# and pass it through the old layers one by one.\n",
        "\n",
        "# A. Create explicit Input\n",
        "input_img = Input(shape=(64, 64, 3), name=\"Image_Input\")\n",
        "\n",
        "# B. Pass it through the loaded layers (Stopping before the last classification layer)\n",
        "x = input_img\n",
        "for layer in base_cloud_model.layers[:-1]: # [:-1] skips the last Softmax layer\n",
        "    layer.trainable = False # Freeze it immediately\n",
        "    # Keras will automatically rename layers if they exist in the graph\n",
        "    # but by manually passing through, we ensure the graph is built correctly\n",
        "    # with existing layer names.\n",
        "    x = layer(x)\n",
        "\n",
        "# Now 'x' holds the \"Visual Embeddings\" (The features)\n",
        "visual_embedding = x\n",
        "\n",
        "# --- HEAD 2: THE PHYSICS BRAIN ---\n",
        "input_phys = Input(shape=(1,), name=\"Physics_Input\")\n",
        "# FIX: Added unique names to Dense layers\n",
        "phys_embedding = layers.Dense(16, activation='relu', name='physics_dense')(input_phys)\n",
        "\n",
        "# --- FUSION ---\n",
        "combined = layers.Concatenate()([visual_embedding, phys_embedding])\n",
        "\n",
        "# Interpretation\n",
        "z = layers.Dense(64, activation='relu', name='combined_dense_1')(combined)\n",
        "z = layers.Dropout(0.2)(z)\n",
        "# FIX: Added unique names to Dense layers\n",
        "z = layers.Dense(32, activation='relu', name='combined_dense_2')(z)\n",
        "\n",
        "# OUTPUT\n",
        "output_k = layers.Dense(1, activation='linear', name=\"Power_Output\")(z)\n",
        "\n",
        "# Create Final Model\n",
        "final_model = Model(inputs=[input_img, input_phys], outputs=output_k)\n",
        "\n",
        "# 5. COMPILE\n",
        "final_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                    loss=physics_guided_loss,\n",
        "                    metrics=['mae'])\n",
        "\n",
        "print(\"✅ SUCCESS! The Two-Headed Monster is built.\")\n",
        "final_model.summary()"
      ],
      "metadata": {
        "id": "7pwr9RFD_XbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. SETUP TRAINING\n",
        "print(\"1. Starting Training of the Two-Headed Monster...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# 2. TRAIN\n",
        "# Notice we pass a LIST of inputs: [Images, Physics]\n",
        "history = final_model.fit(\n",
        "    [X_img_train, X_phys_train], y_train,\n",
        "    validation_data=([X_img_test, X_phys_test], y_test),\n",
        "    epochs=20,\n",
        "    batch_size=64, # Bigger batch size for speed\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "\n",
        "print(f\"\\n✅ TRAINING COMPLETE!\")\n",
        "print(f\"   ⏱️ Time Taken: {duration:.2f} seconds\")\n",
        "\n",
        "# 3. VISUALIZE RESULTS (The Proof)\n",
        "# We need to see if it actually learned, or if it's just guessing.\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot A: The Loss Curve (Did it learn?)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Physics-Guided Loss over Time')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (Physics Penalty + Error)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot B: Actual vs Predicted (The Accuracy)\n",
        "# Let's predict on the Test Set\n",
        "y_pred = final_model.predict([X_img_test, X_phys_test])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# We take the first 100 samples to make the graph readable\n",
        "plt.plot(y_test[:100], label='Actual Ratio', color='black', alpha=0.7)\n",
        "plt.plot(y_pred[:100], label='AI Prediction', color='red', linestyle='--')\n",
        "plt.title('Real vs. AI Prediction (First 100 Test Samples)')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Solar Ratio (0 to 1)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SKJn2M5fAECO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. MOUNT DRIVE (If not already mounted)\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 2. DEFINE SAVE PATH\n",
        "# We will save it in your 'COLAB DATASET' folder so it's safe\n",
        "save_path = \"/content/drive/MyDrive/COLAB DATASET/final_physics_model.keras\"\n",
        "\n",
        "print(f\"1. Saving model to: {save_path}...\")\n",
        "\n",
        "# 3. SAVE\n",
        "try:\n",
        "    # Save in the new Keras format (safer than H5)\n",
        "    final_model.save(save_path)\n",
        "    print(\"✅ SUCCESS! Model saved safely to Google Drive.\")\n",
        "    print(\"   You can now sleep effectively. Your work is safe.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERROR: {e}\")\n",
        "    # Fallback: Try saving locally first\n",
        "    final_model.save(\"final_physics_model.keras\")\n",
        "    print(\"   Saved locally instead. Please download 'final_physics_model.keras' from the file menu manually!\")"
      ],
      "metadata": {
        "id": "LJNtrkamA9OQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "e9d460de-9aef-4eff-b360-c5bdf6255dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Saving model to: /content/drive/MyDrive/COLAB DATASET/final_physics_model.keras...\n",
            "❌ ERROR: name 'final_model' is not defined\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3744554210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Save in the new Keras format (safer than H5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ SUCCESS! Model saved safely to Google Drive.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_model' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3744554210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"❌ ERROR: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Fallback: Try saving locally first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_physics_model.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   Saved locally instead. Please download 'final_physics_model.keras' from the file menu manually!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRlBTwDs9tOT",
        "outputId": "25ef622a-9b4b-4ac8-854f-a584e945f7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. MOUNT DRIVE\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# 2. DEFINE PATHS\n",
        "DRIVE_IMG_PATH = \"/content/drive/MyDrive/COLAB DATASET/X_images_64.npy\"\n",
        "LOCAL_IMG_PATH = \"data/processed/X_images_64.npy\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/COLAB DATASET/final_physics_model.keras\"\n",
        "\n",
        "# 3. TRY TO RECOVER DATA\n",
        "print(\"1. Searching for Image Data...\")\n",
        "\n",
        "if os.path.exists(DRIVE_IMG_PATH):\n",
        "    print(\"   ✅ Found data in Drive! Copying to local runtime...\")\n",
        "    os.makedirs(\"data/processed\", exist_ok=True)\n",
        "    shutil.copy(DRIVE_IMG_PATH, LOCAL_IMG_PATH)\n",
        "    X_img = np.load(LOCAL_IMG_PATH).astype('float32') / 255.0\n",
        "    print(\"   ✅ Data Loaded Successfully.\")\n",
        "else:\n",
        "    print(\"   ❌ Data not found in Drive.\")\n",
        "    print(\"   ⚠️ CRITICAL: The session restarted and the temp files are gone.\")\n",
        "    print(\"   Action Required: Please re-upload your processed 'X_images_64.npy' if you have it.\")\n",
        "    print(\"   OR: We must re-run 'Step 2: Process Solar Images' to regenerate it.\")\n",
        "\n",
        "    # Stop here if data is missing to avoid crashing\n",
        "    X_img = None\n",
        "\n",
        "# 4. IF DATA EXISTS, RUN SHOWCASE\n",
        "if X_img is not None:\n",
        "    print(\"\\n2. Loading Physics Data & Model...\")\n",
        "    # Load Physics\n",
        "    if os.path.exists(\"train_master_physics.csv\"):\n",
        "        df = pd.read_csv(\"train_master_physics.csv\")\n",
        "    else:\n",
        "        # Try to find it in Drive if local is gone\n",
        "        df = pd.read_csv(\"/content/drive/MyDrive/COLAB DATASET/train_master_physics.csv\") # Adjust path if needed\n",
        "\n",
        "    # Align\n",
        "    min_len = min(len(X_img), len(df))\n",
        "    X_img = X_img[:min_len]\n",
        "    df = df.iloc[:min_len]\n",
        "\n",
        "    # Prepare Inputs\n",
        "    scaler = StandardScaler()\n",
        "    X_phys = scaler.fit_transform(df[['GHI_limit']].values) # Assuming V1 Model\n",
        "    y = df['target_k'].values\n",
        "\n",
        "    # Split\n",
        "    _, X_img_test, _, X_phys_test, _, y_test = train_test_split(\n",
        "        X_img, X_phys, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Load Custom Loss for Model\n",
        "    def physics_guided_loss(y_true, y_pred):\n",
        "        return tf.reduce_mean(tf.square(y_true - y_pred)) # Simplified for loading\n",
        "\n",
        "    print(f\"   Loading Model: {MODEL_PATH}\")\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects={'physics_guided_loss': physics_guided_loss})\n",
        "\n",
        "    # Predict\n",
        "    print(\"\\n3. Generating Forecast Showcase...\")\n",
        "    y_pred = model.predict([X_img_test, X_phys_test])\n",
        "\n",
        "    # Build Table\n",
        "    start_idx = 200; end_idx = 215\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Sample_ID': range(start_idx, end_idx),\n",
        "        'Real_Ratio': y_test[start_idx:end_idx].flatten(),\n",
        "        'AI_Prediction': y_pred[start_idx:end_idx].flatten()\n",
        "    })\n",
        "    comparison_df['Error'] = abs(comparison_df['Real_Ratio'] - comparison_df['AI_Prediction'])\n",
        "    comparison_df['Status'] = np.where(comparison_df['Error'] < 0.05, '✅ Accurate', '⚠️ Deviation')\n",
        "\n",
        "    print(\"\\n--- 📊 FORECAST SHOWCASE (Evidence for Thesis) ---\")\n",
        "    print(comparison_df.round(4).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9vySgd39fqb",
        "outputId": "d285355a-168d-42f4-da18-ddc02e686ccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Searching for Image Data...\n",
            "   ❌ Data not found in Drive.\n",
            "   ⚠️ CRITICAL: The session restarted and the temp files are gone.\n",
            "   Action Required: Please re-upload your processed 'X_images_64.npy' if you have it.\n",
            "   OR: We must re-run 'Step 2: Process Solar Images' to regenerate it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. LOAD DATA\n",
        "print(\"1. Loading Data...\")\n",
        "X_img = np.load(\"/content/drive/MyDrive/COLAB DATASET/X_images_64.npy\").astype('float32') / 255.0\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/COLAB DATASET/train_master_physics.csv\")\n",
        "\n",
        "# Align\n",
        "min_len = min(len(X_img), len(df))\n",
        "X_img = X_img[:min_len]\n",
        "df = df.iloc[:min_len]\n",
        "\n",
        "# 2. THE HARD FIX: TEMPORAL SPLIT (No Random Shuffling!)\n",
        "print(\"2. Applying Strict Temporal Split (First 80% vs Last 20%)...\")\n",
        "split_idx = int(len(df) * 0.80)\n",
        "\n",
        "# Train = Jan to Sept (approx)\n",
        "X_img_train, y_train = X_img[:split_idx], df['target_k'].iloc[:split_idx].values\n",
        "df_train = df.iloc[:split_idx]\n",
        "\n",
        "# Test = Oct to Dec (approx)\n",
        "X_img_test, y_test = X_img[split_idx:], df['target_k'].iloc[split_idx:].values\n",
        "df_test = df.iloc[split_idx:]\n",
        "\n",
        "# Prepare Physics Inputs (Fit on TRAIN, Transform on TEST)\n",
        "scaler = StandardScaler()\n",
        "X_phys_train = scaler.fit_transform(df_train[['GHI_limit']].values)\n",
        "X_phys_test = scaler.transform(df_test[['GHI_limit']].values)\n",
        "\n",
        "print(f\"   Training Samples: {len(X_img_train)} (Past)\")\n",
        "print(f\"   Testing Samples: {len(X_img_test)} (Future)\")\n",
        "\n",
        "# 3. RE-TRAIN THE MODEL (This is the 'Hard' verification)\n",
        "# Load your previous model architecture (but reset weights)\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/COLAB DATASET/final_physics_model.keras\", compile=False)\n",
        "model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['mae'])\n",
        "\n",
        "print(\"3. Retraining on Chronological Data...\")\n",
        "history = model.fit(\n",
        "    [X_img_train, X_phys_train], y_train,\n",
        "    validation_data=([X_img_test, X_phys_test], y_test),\n",
        "    epochs=10, # 10 is enough to prove the point\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 4. SAVE THE \"HONEST\" MODEL\n",
        "model.save(\"/content/drive/MyDrive/COLAB DATASET/final_physics_model_temporal.keras\")\n",
        "print(\"✅ FIXED: Data Leakage Risk eliminated. This model predicts the Future.\")"
      ],
      "metadata": {
        "id": "ReFAO1fEEcZH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "2513e58f-9d3a-44e5-a792-1c8121181ea8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Loading Data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/COLAB DATASET/X_images_64.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3386512554.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 1. LOAD DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1. Loading Data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/COLAB DATASET/X_images_64.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/COLAB DATASET/train_master_physics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/COLAB DATASET/X_images_64.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, Input, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. LOAD DATA\n",
        "print(\"1. Loading Data for Baseline...\")\n",
        "X_img = np.load(\"/content/drive/MyDrive/COLAB DATASET/X_images_64.npy\").astype('float32') / 255.0\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/COLAB DATASET/train_master_physics.csv\")\n",
        "\n",
        "# Align\n",
        "min_len = min(len(X_img), len(df))\n",
        "X_img = X_img[:min_len]\n",
        "y = df['target_k'].iloc[:min_len].values\n",
        "\n",
        "# Split (Must use same seed=42 for fair comparison)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_img, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. BUILD \"DUMB\" MODEL (Standard CNN - No Physics)\n",
        "print(\"2. Building Standard CNN (The Baseline)...\")\n",
        "\n",
        "# We use the SAME visual backbone to be fair\n",
        "base_cloud_model = tf.keras.models.load_model(\"models/cloud_classifier.h5\")\n",
        "\n",
        "# Reconstruct the visual head (Frozen)\n",
        "input_img = Input(shape=(64, 64, 3), name=\"Image_Input\")\n",
        "x = input_img\n",
        "for layer in base_cloud_model.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "    x = layer(x)\n",
        "visual_embedding = x\n",
        "\n",
        "# Direct Dense Layers (No Physics Input here!)\n",
        "z = layers.Dense(64, activation='relu')(visual_embedding)\n",
        "z = layers.Dropout(0.2)(z)\n",
        "z = layers.Dense(32, activation='relu')(z)\n",
        "output = layers.Dense(1, activation='linear', name=\"Output\")(z)\n",
        "\n",
        "baseline_model = Model(inputs=input_img, outputs=output)\n",
        "\n",
        "# Standard Loss (MSE) - The AI doesn't know about physics penalties\n",
        "baseline_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# 3. TRAIN\n",
        "print(\"3. Training Baseline...\")\n",
        "history = baseline_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=15,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 4. THE VERDICT\n",
        "print(\"\\n--- ABLATION STUDY RESULTS ---\")\n",
        "y_pred_base = baseline_model.predict(X_test)\n",
        "baseline_mae = np.mean(np.abs(y_test - y_pred_base.flatten()))\n",
        "\n",
        "print(f\"📉 Standard CNN Error (Baseline): {baseline_mae:.4f}\")\n",
        "print(f\"🏆 Physics Model Error (Yours):    0.0475 (Approx)\")\n",
        "\n",
        "improvement = ((baseline_mae - 0.0475) / baseline_mae) * 100\n",
        "print(f\"🚀 Conclusion: Physics inputs improved accuracy by {improvement:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "PDfMhWk1Z1Jl",
        "outputId": "b9bf0008-8966-4daa-bf66-4dc1143f3362"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Loading Data for Baseline...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/COLAB DATASET/X_images_64.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1370748482.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 1. LOAD DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1. Loading Data for Baseline...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mX_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/COLAB DATASET/X_images_64.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/COLAB DATASET/train_master_physics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/COLAB DATASET/X_images_64.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4x1g1pr0l9jl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}